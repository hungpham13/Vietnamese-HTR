{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hungpham13/Vietnamese-HTR/blob/main/vietocr_gettingstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPgu4i1yvhub"
      },
      "source": [
        "\n",
        "# Introduction\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/pbcquoc/vietocr/master/image/vietocr.jpg\" width=\"512\" height=\"512\">\n",
        "</p>\n",
        "This notebook describe how you can use VietOcr to train OCR model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEBHav_aljVN",
        "outputId": "57fedd21-4767-4874-b8cf-8dcce47fb245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 948 kB 14.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 280 kB 73.5 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install --quiet vietocr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS1cz5tKxio7"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5JIpythxipL"
      },
      "source": [
        "Change weights to your weights or using default weights from our pretrained model. Path can be url or local file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UjiubW0gxipL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "c556267369cc4a5ab17807bf1978a8a9",
            "d39392ff11334d56acf0e68c17d37ee0",
            "143bda94330f4ff99eac9c7ca7dab72d",
            "faf2bad8ba604ffda41b9fdeff2878cb",
            "a6ed16d8c5484d128f458c6b42a3d86d",
            "47b2f269840145b7ad40c166b5c90f96",
            "9c44e6bc3da5481eab3f9a279a9feaec",
            "c7e5b21201b34d929e8e8160875fe172",
            "8469dd1d2c394b33ad64deeaed26c95a",
            "f841f96a4c4c4a0693f4b636e00c62dc",
            "129f0102b4f94ee78ace293d0b1b18e1"
          ]
        },
        "outputId": "d635e3a4-5508-4e0c-fe99-af58c1021ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\" to /root/.cache/torch/hub/checkpoints/vgg19_bn-c79401a0.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c556267369cc4a5ab17807bf1978a8a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cached Downloading: /root/.cache/gdown/https-COLON--SLASH--SLASH-drive.google.com-SLASH-uc-QUESTION-id-EQUAL-13327Y1tz1ohsm5YZMyXVMPIOjoOA0OaA\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13327Y1tz1ohsm5YZMyXVMPIOjoOA0OaA\n",
            "To: /root/.cache/gdown/tmp6hvuep5h/dl\n",
            "100%|██████████| 152M/152M [00:00<00:00, 170MB/s]\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from vietocr.tool.predictor import Predictor\n",
        "from vietocr.tool.config import Cfg\n",
        "config = Cfg.load_config_from_name('vgg_transformer')\n",
        "\n",
        "# config['weights'] = './weights/transformerocr.pth'\n",
        "# config['weights'] = 'https://drive.google.com/uc?id=1--0gOdyQXIhQArom-bcDE0ZMuUeVvcUj'\n",
        "config['weights'] = 'https://drive.google.com/uc?id=13327Y1tz1ohsm5YZMyXVMPIOjoOA0OaA'\n",
        "config['cnn']['pretrained']=True\n",
        "config['device'] = 'cuda:0'\n",
        "config['predictor']['beamsearch']=False\n",
        "\n",
        "detector = Predictor(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHtnhkpsxipZ",
        "outputId": "53757965-2d80-45c7-8a19-5a3b728587e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uMVd6EBjY4Q0G2IkU5iMOQ34X0bysm0b\n",
            "To: /content/sample.zip\n",
            "100% 306k/306k [00:00<00:00, 138MB/s]\n"
          ]
        }
      ],
      "source": [
        "! gdown --id 1uMVd6EBjY4Q0G2IkU5iMOQ34X0bysm0b\n",
        "! unzip  -qq -o sample.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "B95BBXNExipj",
        "outputId": "70dc7ff7-0bc4-4cf3-f8a6-700dfc3583c1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e60ea3e88b33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./Vietnamese-HTR/data/0825_DataSamples_1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Vietnamese-HTR/data/0825_DataSamples_1'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "dir = \"./Vietnamese-HTR/data/0825_DataSamples_1\"\n",
        "for img in os.listdir(dir):\n",
        "    img = os.path.join(dir,img)\n",
        "    if img[-4:] == \"json\": continue\n",
        "    img = Image.open(img)\n",
        "    plt.imshow(img)\n",
        "    s = detector.predict(img, return_prob=True)\n",
        "    print(s)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9zjgHwN2vuC"
      },
      "source": [
        "# Prepare the dataset\n",
        "Very large dataset (1M images)\n",
        "- en_00: 100000\n",
        "- en_01: 100000\n",
        "- vi_00: 100000 \n",
        "- vi_01: 100000 \n",
        "- Các ảnh có màu giấy (màu nền) khá giống nhau\n",
        "- meta: 144000 \n",
        "- random: 50000\n",
        "\n",
        "Vietnamese Handwritten Address Dataset\n",
        "- train: 1829\n",
        "- test: 549\n",
        "\n",
        "MCOCR2021\n",
        "- train: 1546\n",
        "- test: 390\n",
        "\n",
        "BKAI Scene Text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crawl datasets"
      ],
      "metadata": {
        "id": "ZeaPtCPoEZRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Vietnamese Handwritten Address Dataset"
      ],
      "metadata": {
        "id": "E-WlAas-EUWV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XxMTP2s4-nNY",
        "outputId": "14c77cf3-0769-4d30-874a-4f3588c42902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Vietnamese-HTR' already exists and is not an empty directory.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r ./Vietnamese-HTR/requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r ./Vietnamese-HTR/requirements.txt (line 4)) (1.21.6)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r ./Vietnamese-HTR/requirements.txt (line 6)) (7.1.2)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r ./Vietnamese-HTR/requirements.txt (line 9)) (1.12.0+cu113)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r ./Vietnamese-HTR/requirements.txt (line 10)) (0.13.0+cu113)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from -r ./Vietnamese-HTR/requirements.txt (line 11)) (4.64.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from -r ./Vietnamese-HTR/requirements.txt (line 12)) (1.3.5)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r ./Vietnamese-HTR/requirements.txt (line 13)) (0.11.2)\n",
            "Requirement already satisfied: scikit-learn>=0.19.2 in /usr/local/lib/python3.7/dist-packages (from -r ./Vietnamese-HTR/requirements.txt (line 14)) (1.0.2)\n",
            "Collecting pytorch-lightning>=1.6.3\n",
            "  Downloading pytorch_lightning-1.6.5-py3-none-any.whl (585 kB)\n",
            "\u001b[K     |████████████████████████████████| 585 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting torchmetrics~=0.8.2\n",
            "  Downloading torchmetrics-0.8.2-py3-none-any.whl (409 kB)\n",
            "\u001b[K     |████████████████████████████████| 409 kB 55.9 MB/s \n",
            "\u001b[?25hCollecting torchinfo~=1.6.6\n",
            "  Downloading torchinfo-1.6.6-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r ./Vietnamese-HTR/requirements.txt (line 3)) (1.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r ./Vietnamese-HTR/requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r ./Vietnamese-HTR/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r ./Vietnamese-HTR/requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->-r ./Vietnamese-HTR/requirements.txt (line 9)) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.8.1->-r ./Vietnamese-HTR/requirements.txt (line 10)) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4->-r ./Vietnamese-HTR/requirements.txt (line 12)) (2022.1)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn>=0.11.0->-r ./Vietnamese-HTR/requirements.txt (line 13)) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.2->-r ./Vietnamese-HTR/requirements.txt (line 14)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.2->-r ./Vietnamese-HTR/requirements.txt (line 14)) (3.1.0)\n",
            "Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (3.17.3)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 74.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (2.8.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (21.3)\n",
            "Collecting pyDeprecate>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Collecting PyYAML>=5.4\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 55.4 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 49.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (57.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (1.47.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (1.1.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.8.1->-r ./Vietnamese-HTR/requirements.txt (line 10)) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.8.1->-r ./Vietnamese-HTR/requirements.txt (line 10)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.8.1->-r ./Vietnamese-HTR/requirements.txt (line 10)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.8.1->-r ./Vietnamese-HTR/requirements.txt (line 10)) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (3.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 77.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (21.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.6.3->-r ./Vietnamese-HTR/requirements.txt (line 15)) (2.1.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 68.4 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyDeprecate, fsspec, aiohttp, torchmetrics, PyYAML, torchinfo, pytorch-lightning\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.3.0 fsspec-2022.5.0 multidict-6.0.2 pyDeprecate-0.3.2 pytorch-lightning-1.6.5 torchinfo-1.6.6 torchmetrics-0.8.2 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!git clone https://github.com/hungpham13/Vietnamese-HTR.git\n",
        "!pip install -r ./Vietnamese-HTR/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BKAI Scene Text"
      ],
      "metadata": {
        "id": "HtSKmXVAEFFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown https://drive.google.com/uc?id=1UUQhNvzgpZy7zXBFQp0Qox-BBjunZ0ml\n",
        "! gdown https://drive.google.com/uc?id=1_Z4zY2Wk7vtxepUhUzttfddkM2b9cap2\n",
        "! gdown https://drive.google.com/uc?id=1AhEwdTOxByNiHLfZcxFm83ZtPivxGJlS\n",
        "! gdown https://drive.google.com/uc?id=1PLW2eGsMSVXZahZ7GvEz5Kr1uvyLnxbx\n",
        "! unzip -qq -o ./vietnamese_original.zip -d ./scencetext_dataset\n",
        "! unzip -qq -o ./train_imgs.zip -d ./scencetext_dataset\n",
        "! unzip -qq -o ./train_gt.zip -d ./scencetext_dataset\n",
        "! unzip -qq -o ./public_test_imgs.zip -d ./scencetext_dataset\n",
        "! rm ./vietnamese_original.zip\n",
        "! rm ./train_imgs.zip\n",
        "! rm ./train_gt.zip\n",
        "! rm ./public_test_imgs.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NO3sHH2B3gx",
        "outputId": "5515e16a-1898-42ad-e18f-5b77ebe166ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UUQhNvzgpZy7zXBFQp0Qox-BBjunZ0ml\n",
            "To: /content/vietnamese_original.zip\n",
            "100% 1.05G/1.05G [00:06<00:00, 153MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_Z4zY2Wk7vtxepUhUzttfddkM2b9cap2\n",
            "To: /content/train_imgs.zip\n",
            "100% 96.2M/96.2M [00:01<00:00, 88.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AhEwdTOxByNiHLfZcxFm83ZtPivxGJlS\n",
            "To: /content/train_gt.zip\n",
            "100% 166k/166k [00:00<00:00, 83.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PLW2eGsMSVXZahZ7GvEz5Kr1uvyLnxbx\n",
            "To: /content/public_test_imgs.zip\n",
            "100% 68.2M/68.2M [00:00<00:00, 77.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MCOCR 2021"
      ],
      "metadata": {
        "id": "77ppwG-ZD9FT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown https://drive.google.com/uc?id=1FgCnaotis_NKTbgXdviTTDhrM2AvsdTT\n",
        "! gdown https://drive.google.com/uc?id=1xL1NHwf00WWJUsPcS4plZEDmTPQqGUsW\n",
        "!unzip -qq -o ./mcocr2021_private_test_data.zip -d ./mc_ocr\n",
        "!unzip -qq -o ./mcocr2021_public_train_test_data.zip -d ./mc_ocr\n",
        "!rm mcocr2021_private_test_data.zip\n",
        "!rm mcocr2021_public_train_test_data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbqYyM9l7xbB",
        "outputId": "1c9abbc9-3103-4b51-a625-42949a3b6f79"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FgCnaotis_NKTbgXdviTTDhrM2AvsdTT\n",
            "To: /content/mcocr2021_public_train_test_data.zip\n",
            "100% 355M/355M [00:05<00:00, 68.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xL1NHwf00WWJUsPcS4plZEDmTPQqGUsW\n",
            "To: /content/mcocr2021_private_test_data.zip\n",
            "100% 89.9M/89.9M [00:00<00:00, 111MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VietOCR 1M"
      ],
      "metadata": {
        "id": "CZDVONHVD__G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ZEBWugoTnvy0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4f582ac-2a81-4236-f997-e14eeb1c91d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vi9zY9_jnpqTGkBJ-eSt7tnnWEYTHuFr\n",
            "To: /content/ocr_dataset.zip\n",
            "100% 7.71G/7.71G [01:18<00:00, 98.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19QU4VnKtgm3gf0Uw_N2QKSquW1SQ5JiE\n",
            "To: /content/data_line.zip\n",
            "100% 61.2M/61.2M [00:00<00:00, 368MB/s]\n"
          ]
        }
      ],
      "source": [
        "! gdown https://drive.google.com/uc?id=1vi9zY9_jnpqTGkBJ-eSt7tnnWEYTHuFr\n",
        "! gdown https://drive.google.com/uc?id=19QU4VnKtgm3gf0Uw_N2QKSquW1SQ5JiE\n",
        "! unzip -qq -o ./ocr_dataset.zip -d ./ocr_dataset\n",
        "! unzip -qq -o ./data_line.zip -d ./ocr_dataset\n",
        "! rm -rf ./ocr_dataset/InkData_line_processed/\n",
        "! rm data_line.zip\n",
        "! rm ocr_dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploit, reorganize to VietOCR training format"
      ],
      "metadata": {
        "id": "mLtuBKCeEjCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -a ./ocr_dataset/meta"
      ],
      "metadata": {
        "id": "IrdRFBBh3XK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize some images"
      ],
      "metadata": {
        "id": "bglrwHsA3KMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = [30.00, 30.50]\n",
        "plt.rcParams[\"figure.autolayout\"] = True\n",
        "im = plt.imread(\"/content/mc_ocr/mcocr_public_train_test_shared_data/mcocr_train_data/train_images/mcocr_public_145013ddcph.jpg\")\n",
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(im)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v5DFeeui3IxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "# len([path for path in Path('./ocr_dataset').rglob('*.jpg')])\n",
        "# len([path for path in Path('./mcocr_public_train_test_shared_data').rglob('*.jpg')])\n",
        "len([path for path in Path('./Vietnamese-HTR/data/0916_Data_Samples_2').rglob('*.png')])\n",
        "\n",
        "# print(len(glob.glob(\"./ocr_dataset/en_00/*.txt\")))\n",
        "# print(len(glob.glob(\"./ocr_dataset/en_01/*.txt\")))"
      ],
      "metadata": {
        "id": "ofoKsFpX37rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YFqpZKzDheZ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "root = \"/content/Vietnamese-HTR/data/\"\n",
        "\n",
        "def create_vietocr_annote(root, img_dir_name, output_name):\n",
        "    #read the file\n",
        "    with open(root + img_dir_name + \"/labels.json\") as file:\n",
        "        labels = json.load(file)\n",
        "    data = []\n",
        "    for img_name in labels:\n",
        "        data.append(f\"{img_dir_name}/{img_name}\\t{labels[img_name]}\\n\")\n",
        "\n",
        "\n",
        "    #write back the file\n",
        "    with open(root + output_name,\"w\") as f:\n",
        "        f.writelines(data)\n",
        "create_vietocr_annote(root, \"0916_Data_Samples_2\", \"train_annotation.txt\")\n",
        "create_vietocr_annote(root, \"1015_Private_Test\", \"test_annotation.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model exploration"
      ],
      "metadata": {
        "id": "YBK0mt3sSvFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from vietocr.tool.translate import translate\n",
        "class OCR_Detector(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(OCR_Detector, self).__init__()\n",
        "        self.model = model\n",
        "    def forward(self, img):\n",
        "        return translate(img, self.model)\n"
      ],
      "metadata": {
        "id": "3ejgzfmDYSis"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detector.model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIwkWjV-Vu98",
        "outputId": "23ef063e-b917-4c6a-f7fc-bcb40d47b3d0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VietOCR(\n",
              "  (cnn): CNN(\n",
              "    (model): Vgg(\n",
              "      (features): Sequential(\n",
              "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU(inplace=True)\n",
              "        (6): AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0)\n",
              "        (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (9): ReLU(inplace=True)\n",
              "        (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (12): ReLU(inplace=True)\n",
              "        (13): AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0)\n",
              "        (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (16): ReLU(inplace=True)\n",
              "        (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (19): ReLU(inplace=True)\n",
              "        (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (22): ReLU(inplace=True)\n",
              "        (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (25): ReLU(inplace=True)\n",
              "        (26): AvgPool2d(kernel_size=[2, 1], stride=[2, 1], padding=0)\n",
              "        (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (29): ReLU(inplace=True)\n",
              "        (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (32): ReLU(inplace=True)\n",
              "        (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (35): ReLU(inplace=True)\n",
              "        (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (38): ReLU(inplace=True)\n",
              "        (39): AvgPool2d(kernel_size=[2, 1], stride=[2, 1], padding=0)\n",
              "        (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (42): ReLU(inplace=True)\n",
              "        (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (45): ReLU(inplace=True)\n",
              "        (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (48): ReLU(inplace=True)\n",
              "        (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (51): ReLU(inplace=True)\n",
              "        (52): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.5, inplace=False)\n",
              "      (last_conv_1x1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (transformer): LanguageTransformer(\n",
              "    (embed_tgt): Embedding(233, 256)\n",
              "    (pos_enc): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (encoder): TransformerEncoder(\n",
              "        (layers): ModuleList(\n",
              "          (0): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout1): Dropout(p=0.1, inplace=False)\n",
              "            (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout1): Dropout(p=0.1, inplace=False)\n",
              "            (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout1): Dropout(p=0.1, inplace=False)\n",
              "            (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (3): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout1): Dropout(p=0.1, inplace=False)\n",
              "            (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (4): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout1): Dropout(p=0.1, inplace=False)\n",
              "            (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (5): TransformerEncoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout1): Dropout(p=0.1, inplace=False)\n",
              "            (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): TransformerDecoder(\n",
              "        (layers): ModuleList(\n",
              "          (0): TransformerDecoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (multihead_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout1): Dropout(p=0.1, inplace=False)\n",
              "            (dropout2): Dropout(p=0.1, inplace=False)\n",
              "            (dropout3): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): TransformerDecoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (multihead_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout1): Dropout(p=0.1, inplace=False)\n",
              "            (dropout2): Dropout(p=0.1, inplace=False)\n",
              "            (dropout3): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): TransformerDecoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (multihead_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout1): Dropout(p=0.1, inplace=False)\n",
              "            (dropout2): Dropout(p=0.1, inplace=False)\n",
              "            (dropout3): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (3): TransformerDecoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (multihead_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout1): Dropout(p=0.1, inplace=False)\n",
              "            (dropout2): Dropout(p=0.1, inplace=False)\n",
              "            (dropout3): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (4): TransformerDecoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (multihead_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout1): Dropout(p=0.1, inplace=False)\n",
              "            (dropout2): Dropout(p=0.1, inplace=False)\n",
              "            (dropout3): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (5): TransformerDecoderLayer(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (multihead_attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "            )\n",
              "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout1): Dropout(p=0.1, inplace=False)\n",
              "            (dropout2): Dropout(p=0.1, inplace=False)\n",
              "            (dropout3): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (fc): Linear(in_features=256, out_features=233, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(OCR_Detector(detector.model), (4,3,118,2202))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5Im1Lo7fSuIu",
        "outputId": "380f953d-f7aa-4ad1-88ab-9911f3d52e98"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-694e5b8fe579>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/vietocr/tool/translate.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(img, model, max_seq_length, sos_token, eos_token)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/vietocr/model/seqmodel/transformer.py\u001b[0m in \u001b[0;36mforward_encoder\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/vietocr/model/seqmodel/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3850) must match the size of tensor b (1024) at non-singleton dimension 0",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-91ab1d4a5486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchinfo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOCR_Detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m118\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2202\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     )\n\u001b[1;32m    217\u001b[0m     summary_list = forward_pass(\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_forward_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     )\n\u001b[1;32m    220\u001b[0m     \u001b[0mformatting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFormattingOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;34m\"Failed to run torchinfo. See above stack traces for more details. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0;34mf\"Executed layers up to: {executed_layers}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         ) from e\n\u001b[0m\u001b[1;32m    309\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [CNN: 2, Vgg: 3, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, AvgPool2d: 5, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, AvgPool2d: 5, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, AvgPool2d: 5, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, AvgPool2d: 5, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, AvgPool2d: 5, Dropout: 4, Conv2d: 4]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1lxSkEj20y0"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MWgUSotv1sN"
      },
      "source": [
        "\n",
        "\n",
        "1.   Load your config\n",
        "2.   Train model using your dataset above\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuzRB0rxwC3m"
      },
      "source": [
        "Load the default config, we adopt VGG for image feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMwREzEvm_jd"
      },
      "outputs": [],
      "source": [
        "from vietocr.tool.config import Cfg\n",
        "from vietocr.model.trainer import Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oKRCu2ewNE4"
      },
      "source": [
        "## Change the config \n",
        "\n",
        "* *data_root*: the folder save your all images\n",
        "* *train_annotation*: path to train annotation\n",
        "* *valid_annotation*: path to valid annotation\n",
        "* *print_every*: show train loss at every n steps\n",
        "* *valid_every*: show validation loss at every n steps\n",
        "* *iters*: number of iteration to train your model\n",
        "* *export*: export weights to folder that you can use for inference\n",
        "* *metrics*: number of sample in validation annotation you use for computing full_sequence_accuracy, for large dataset it will take too long, then you can reuduce this number\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Config 1: Transformer"
      ],
      "metadata": {
        "id": "L9Owxit52UVb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56VBD-Xy_ztj"
      },
      "outputs": [],
      "source": [
        "config = Cfg.load_config_from_name('vgg_transformer')\n",
        "config2 = Cfg.load_config_from_name('vgg_seq2seq')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PZ1h1icu-Hb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceKcT5eOnJ1G"
      },
      "outputs": [],
      "source": [
        "#config['vocab'] = 'aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ '\n",
        "\n",
        "root = \"/content/Vietnamese-HTR/data/\"\n",
        "dataset_params = {\n",
        "    'name':'hw',\n",
        "    'data_root': root,\n",
        "    'train_annotation':'train_annotation.txt',\n",
        "    'valid_annotation':'test_annotation.txt'\n",
        "}\n",
        "\n",
        "params = {\n",
        "         'print_every':200,\n",
        "         'valid_every':15*200,\n",
        "          'iters':10000,\n",
        "          'checkpoint':'.drive/MyDrive/checkpoint/transformerocr_checkpoint.pth',    \n",
        "          'export':'./drive/MyDrive/weights/transformerocr.pth',\n",
        "          'metrics': 10000\n",
        "         }\n",
        "\n",
        "dataloader_params = {'num_workers': 2, 'pin_memory': True}\n",
        "config['trainer'].update(params)\n",
        "config['dataset'].update(dataset_params)\n",
        "config['dataloader'].update(dataloader_params)\n",
        "config['device'] = 'cuda'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufc86K0axiqN"
      },
      "source": [
        "you can change any of these params in this full list below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwHpqqQEnHv1"
      },
      "outputs": [],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Config 2: Seq2seq"
      ],
      "metadata": {
        "id": "-aCoPms-2DyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config2"
      ],
      "metadata": {
        "id": "t5g9cZd12Bkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dxTXpqa3Hd3"
      },
      "source": [
        "## Training\n",
        "You should train model from our pretrained "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtOyT8Cpo1gl"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(config, pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvG8aEOlBmVg"
      },
      "source": [
        "Save model configuration for inference, load_config_from_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Potem8SsojYM"
      },
      "outputs": [],
      "source": [
        "trainer.config.save('config.yml')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyL8nmmTxiqV"
      },
      "source": [
        "Visualize your dataset to check data augmentation is appropriate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OET9Vh8XxiqV"
      },
      "outputs": [],
      "source": [
        "trainer.visualize_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EkP2h64xiqZ"
      },
      "source": [
        "Train now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpZEz_DPpV6y"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iig6gpyb3jtz"
      },
      "source": [
        "Visualize prediction from our trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zeai5W02qXA9"
      },
      "outputs": [],
      "source": [
        "trainer.visualize_prediction()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bAXlHJv3ryW"
      },
      "source": [
        "Compute full seq accuracy for full valid dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM806O42q_aT"
      },
      "outputs": [],
      "source": [
        "trainer.precision()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "vietocr_gettingstart.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c556267369cc4a5ab17807bf1978a8a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d39392ff11334d56acf0e68c17d37ee0",
              "IPY_MODEL_143bda94330f4ff99eac9c7ca7dab72d",
              "IPY_MODEL_faf2bad8ba604ffda41b9fdeff2878cb"
            ],
            "layout": "IPY_MODEL_a6ed16d8c5484d128f458c6b42a3d86d"
          }
        },
        "d39392ff11334d56acf0e68c17d37ee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47b2f269840145b7ad40c166b5c90f96",
            "placeholder": "​",
            "style": "IPY_MODEL_9c44e6bc3da5481eab3f9a279a9feaec",
            "value": "100%"
          }
        },
        "143bda94330f4ff99eac9c7ca7dab72d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7e5b21201b34d929e8e8160875fe172",
            "max": 574769405,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8469dd1d2c394b33ad64deeaed26c95a",
            "value": 574769405
          }
        },
        "faf2bad8ba604ffda41b9fdeff2878cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f841f96a4c4c4a0693f4b636e00c62dc",
            "placeholder": "​",
            "style": "IPY_MODEL_129f0102b4f94ee78ace293d0b1b18e1",
            "value": " 548M/548M [00:02&lt;00:00, 230MB/s]"
          }
        },
        "a6ed16d8c5484d128f458c6b42a3d86d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47b2f269840145b7ad40c166b5c90f96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c44e6bc3da5481eab3f9a279a9feaec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7e5b21201b34d929e8e8160875fe172": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8469dd1d2c394b33ad64deeaed26c95a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f841f96a4c4c4a0693f4b636e00c62dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "129f0102b4f94ee78ace293d0b1b18e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}